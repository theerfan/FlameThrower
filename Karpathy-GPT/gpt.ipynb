{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT = Generative Pre-Trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a string\n",
    "with open(\"tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Convert the text into a sequence of integers\n",
    "encoder = lambda s: [stoi[c] for c in s]\n",
    "decoder = lambda s: ''.join([itos[c] for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encoder(text), dtype=torch.int64)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "n_train = int(0.9 * data_size)\n",
    "train_data = data[:n_train]\n",
    "test_data = data[n_train:]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to feed the entire sequence into the model at once. \\\n",
    "Instead, we'll feed it a small chunk at a time, and then let it continue predicting the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sometimes called the chunk size or batch size.\n",
    "block_size = 8\n",
    "\n",
    "# We will actually separate block_size + 1 characters at a time. \n",
    "# Look at it this way:\n",
    "\n",
    "# For block[0],    we will predict block[1], \n",
    "# For block[0, 1], we will predict block[2], \n",
    "# and so on. \n",
    "\n",
    "# Which means we need to have block[8] as target for block[0, ... , 7]\n",
    "\n",
    "# This is not just done for efficiency, but also to make sure that the \n",
    "# Transformer gets used to seeing contexts as little as 1 character long \n",
    "# and as long as block_size characters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [18], the target is 47.\n",
      "When input is [18, 47], the target is 56.\n",
      "When input is [18, 47, 56], the target is 57.\n",
      "When input is [18, 47, 56, 57], the target is 58.\n",
      "When input is [18, 47, 56, 57, 58], the target is 1.\n",
      "When input is [18, 47, 56, 57, 58, 1], the target is 15.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15], the target is 47.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15, 47], the target is 58.\n"
     ]
    }
   ],
   "source": [
    "x = train_data[:block_size]\n",
    "y = train_data[1 : block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[: t + 1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context.tolist()}, the target is {target}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's one more thing to care about and that's the batch dimension.\n",
    "# We want to be able to feed in a batch of sequences at a time (as a tensor) to the GPU\n",
    "# and have it process all of them in parallel for a speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a seed to make sure the same pseudo-random sequence is generated every time we run this cell\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4      # How many independent sequences to process in parallel\n",
    "block_size = 8      # Maximum context length for predictions\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # generate a small batch of data of inputs `x` and targets `y`\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    # stochastic sampling of the data, ix = indices\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "When input is [24], the target is 43.\n",
      "When input is [24, 43], the target is 58.\n",
      "When input is [24, 43, 58], the target is 5.\n",
      "When input is [24, 43, 58, 5], the target is 57.\n",
      "When input is [24, 43, 58, 5, 57], the target is 1.\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39.\n",
      "When input is [44], the target is 53.\n",
      "When input is [44, 53], the target is 56.\n",
      "When input is [44, 53, 56], the target is 1.\n",
      "When input is [44, 53, 56, 1], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58], the target is 46.\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52], the target is 58.\n",
      "When input is [52, 58], the target is 1.\n",
      "When input is [52, 58, 1], the target is 58.\n",
      "When input is [52, 58, 1, 58], the target is 46.\n",
      "When input is [52, 58, 1, 58, 46], the target is 39.\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46.\n",
      "When input is [25], the target is 17.\n",
      "When input is [25, 17], the target is 27.\n",
      "When input is [25, 17, 27], the target is 10.\n",
      "When input is [25, 17, 27, 10], the target is 0.\n",
      "When input is [25, 17, 27, 10, 0], the target is 21.\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Print an example batch\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, : t + 1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}.\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Logit function, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity. The function is an inverse to the sigmoid function that limits values between 0 and 1 across the Y-axis, rather than the X-axis.\n",
    "\n",
    "$$\n",
    "logit(p) = \\log(\\frac{p}{1-p})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "        # The vocab_size * vocab_size matrix is the embedding table\n",
    "        # which means that we every word in the vocabulary is going to be represented by a vector of vocab_size length\n",
    "        # More on word embeddings: https://www.tensorflow.org/text/guide/word_embeddings\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor:\n",
    "\n",
    "        # idx and targets are both (Batch, Time) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channel)\n",
    "\n",
    "        # When we pass our input through the embedding table, \n",
    "        # every single integer in our input is going to refer to this embedding table\n",
    "        # is going to pluck out a row from this embedding table corresponding to that integer (as an index)\n",
    "        # In this case, Batch = 4, Time = 8, Channel = 65 (vocab_size)\n",
    "\n",
    "        # Logits are the scores for the next character in the sequence\n",
    "        # As you see, this is a bi-gram, which means that the next character is predicted\n",
    "        # Only based the current character, and not the entire sequence\n",
    "\n",
    "        # Aka negative log likelihood loss\n",
    "        # loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The above code won't work, because we have a tensor in the shape of (B, T, C)\n",
    "        # But Pytorch's cross entropy expects a tensor of shape (B * T, C)\n",
    "        # So basically we need to flatten the input tensor and the target tensor into a 1D tensor\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5262, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, because initally our model should equally predict any character as the next character\n",
    "# Our log likelihood loss should be around -ln(1/65) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'JgC.JZWqUkpdtkSpmzjM-,RqzgaN?vC:hgjnAnBZDga-APqGUH!WdCbIb;$DefOYbEvcaKGMmnO'q$KdS-'ZH\n",
      ".YSqr'X!Q! d;\n"
     ]
    }
   ],
   "source": [
    "# We set our initial token to be 0, which is the newline character in our vocabulary\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# We have one batch and the [0] is picking the first batch\n",
    "print(decoder(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "# The model is obviously not trained so it's just spitting out random characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 3.6967320442199707\n",
      "Step 1000, loss = 3.0273618698120117\n",
      "Step 2000, loss = 2.821552276611328\n",
      "Step 3000, loss = 2.547393798828125\n",
      "Step 4000, loss = 2.519805908203125\n",
      "Step 5000, loss = 2.529714584350586\n",
      "Step 6000, loss = 2.542541980743408\n",
      "Step 7000, loss = 2.5140950679779053\n",
      "Step 8000, loss = 2.5017194747924805\n",
      "Step 9000, loss = 2.5214755535125732\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 10000\n",
    "\n",
    "for steps in range(max_steps):\n",
    "    # get a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # get the model predictions and loss\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # zero out the gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Step {steps}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CKELOresm, bur stthakls,\n",
      "Ther layo-then ha nleincede jahe\n",
      "DZW:\n",
      "Gothe s kendwepive.\n",
      "FAnorereroldghmig ppu:\n",
      "Co nlllinger hus;\n",
      "aver his, t towis t s ng,\n",
      "ANE: foratreaisplblthriat, otimust hiny ille, yomeON p, IN I ckist vemo th.\n",
      "Dieathy al hi?\n",
      "Fo'd ha s?\n",
      "ARS:\n",
      "Semnd thinghy.\n",
      "IORDitwint sth! mine actwis \n"
     ]
    }
   ],
   "source": [
    "# We set our initial token to be 0, which is the newline character in our vocabulary\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# We have one batch and the [0] is picking the first batch\n",
    "print(decoder(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is obviously not shakespeare, but it's a lot better than random characters!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# consider the following toy example:\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 8, 2\n",
    "# randn = normal distribution with mean 0 and variance 1\n",
    "x = torch.randn(B, T, C)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, so now we want to bring out some information from the tokens\n",
    "# that come before the current token at each step and use this information\n",
    "# in predicting the next token\n",
    "# The easiest way to do this is to average out all the embedding vectors\n",
    "# of the tokens that come before the current token (with the current token included)\n",
    "\n",
    "\n",
    "# bow = bag of words \n",
    "# bag here means that it's an average of the word vectors\n",
    "xbow = torch.zeros((B, T, C))\n",
    "\n",
    "# We want x[b, t] = mean_{i <= t} x[b, i] \n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        #                 [0, t]\n",
    "        xbow[b, t] = x[b, :t+1].mean(dim=0)\n",
    "\n",
    "# This thing uses `for` loops, which is not good for performance\n",
    "# and is used for illustration purposes only\n",
    "# We can do this much more efficiently using matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = \n",
      " tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]]) \n",
      "\n",
      "b = \n",
      " tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]]) \n",
      "\n",
      "c = \n",
      " tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of doing averaging using matrix multiplication\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Torch.tril gives us a lower triangular matrix of its input\n",
    "# (the upper triangular part is zeroed out)\n",
    "a = torch.tril(torch.ones(3, 3))\n",
    "\n",
    "# dim = 1 means that we are summing over the rows\n",
    "# This is for doing the averaging part \n",
    "a = a / torch.sum(a, dim=1, keepdim=True)\n",
    "\n",
    "b = torch.randint(low=0, high=10, size=(3, 2)).float()\n",
    "\n",
    "c = a @ b\n",
    "\n",
    "print(f\"a = \\n {a} \\n\")\n",
    "print(f\"b = \\n {b} \\n\")\n",
    "print(f\"c = \\n {c} \\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can see that this gives us the averages in incremental fashion:\n",
    "# The first row of c is the average of the first row of b\n",
    "# The second row of c is the average of the first and second row of b\n",
    "# The third row of c is the average of the first, second and third row of b\n",
    "# and so on\n",
    "\n",
    "# So now let's go back and do this for our toy example\n",
    "\n",
    "# wei is short for weights\n",
    "wei = torch.tril(torch.ones(T, T))\n",
    "wei = wei / torch.sum(wei, dim=1, keepdim=True)\n",
    "\n",
    "xbow2 = wei @ x # (T, T) @ (B, T, C) -> (B, T, C)\n",
    "# Pytorch will see that these shapes are incompatiable and will automatically\n",
    "# broadcast the first matrix to be (B, T, T) and then do the matrix multiplication\n",
    "# So it's a (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "# First dim is the same, so the second and third dim are multiplied\n",
    "# and the result is broadcasted to the first dim.\n",
    "# Something like this:\n",
    "# (B, (T, T)) @ (B, (T, C)) = (B, (T, T) @ (T, C)) = (B, (T, C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 3: Use softmax!\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "wei = torch.zeros((T, T))\n",
    "# In wei, replace the elements where the corresponding element in tril is 0 with -inf\n",
    "# By corresponding, meaning the same index because they have the same shape\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "# Softmax gets the sum of e^{x} for x in each row and divides each element by that sum\n",
    "# So the sum of each row is 1. (e^{0} = 1, e^{-inf} = 0)\n",
    "wei = F.softmax(wei, dim=-1) \n",
    "xbow3 = wei @ x\n",
    "\n",
    "torch.allclose(xbow3, xbow2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funny thing to mention: \\\n",
    "You could see Andrej's lighting get dimmer and dimmer as time went by.\n",
    "He finally went to sleep at 1:01:57 in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model some more\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # We have changed our channel size to be n_embed, but we wanted to get logits\n",
    "        # in such a way that the logits are in the same shape as the vocabulary\n",
    "        # because they are the probabilities of each token in the vocabular for being the next token\n",
    "        # So we add a linear layer to transform the channel size to be the same as the vocab size\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        # lm = language model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "\n",
    "        # So now, the x will not only hold the token identity, but also the position of the token\n",
    "        # It is currently not that useful because we're only using the bigram model,\n",
    "        # so it's all translation invariant at this stage. (Doesn't matter _where_ the token is)\n",
    "        # But this will be useful when we use the transformer model (self-attention)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
       "         [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
       "         [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1687, 0.8313, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2477, 0.0514, 0.7008, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4410, 0.0957, 0.3747, 0.0887, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0069, 0.0456, 0.0300, 0.7748, 0.1427, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0660, 0.0892, 0.0413, 0.6316, 0.1649, 0.0069, 0.0000, 0.0000],\n",
       "         [0.0396, 0.2288, 0.0090, 0.2000, 0.2061, 0.1949, 0.1217, 0.0000],\n",
       "         [0.3650, 0.0474, 0.0767, 0.0293, 0.3084, 0.0784, 0.0455, 0.0493]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4820, 0.5180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1705, 0.4550, 0.3745, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0074, 0.7444, 0.0477, 0.2005, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.8359, 0.0416, 0.0525, 0.0580, 0.0119, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1195, 0.2061, 0.1019, 0.1153, 0.1814, 0.2758, 0.0000, 0.0000],\n",
       "         [0.0065, 0.0589, 0.0372, 0.3063, 0.1325, 0.3209, 0.1378, 0.0000],\n",
       "         [0.1416, 0.1519, 0.0384, 0.1643, 0.1207, 0.1254, 0.0169, 0.2408]],\n",
       "\n",
       "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.6369, 0.3631, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.2586, 0.7376, 0.0038, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.4692, 0.3440, 0.1237, 0.0631, 0.0000, 0.0000, 0.0000, 0.0000],\n",
       "         [0.1865, 0.4680, 0.0353, 0.1854, 0.1248, 0.0000, 0.0000, 0.0000],\n",
       "         [0.0828, 0.7479, 0.0017, 0.0735, 0.0712, 0.0228, 0.0000, 0.0000],\n",
       "         [0.0522, 0.0517, 0.0961, 0.0375, 0.1024, 0.5730, 0.0872, 0.0000],\n",
       "         [0.0306, 0.2728, 0.0333, 0.1409, 0.1414, 0.0582, 0.0825, 0.2402]]],\n",
       "       grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we get to the most important part of the tutorial\n",
    "# The crux of 'self-attention'.\n",
    "\n",
    "# Version 4: Self-attention!\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "# Initializing the input vector\n",
    "B, T, C = 4, 8, 32\n",
    "x = torch.randn(B, T, C)\n",
    "\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# The weights are the initial \"affinities\" between tokens,\n",
    "# When we initialize them to be 0, we will get the uniform numbers we got before\n",
    "# Now, we don't want this to be this way, because some tokens will find different tokens\n",
    "# more or less interesting, and we want that to be data-dependent. \n",
    "# e.g. a wovel might look for consonants in its past and would want that information to flow towards it\n",
    "# This is the problem that self-attention solves! (more details below)\n",
    "wei = torch.zeros((T, T))\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "out = wei @ x\n",
    "\n",
    "# Now, the way that self-attention solves this is the following:\n",
    "# Ever single node/token at each position will \"emit\" two vectors:\n",
    "# 1. A query vector\n",
    "# 2. A key vector\n",
    "# The query vector is, roughly speaking, what am I looking for?\n",
    "# The key vector is, roughly speaking, what do I contain?\n",
    "# And the way we get affinities between tokens is by taking the dot product of the query and key vectors\n",
    "# So my query, dot producted will keys of all the other tokens now becomes the weights `wei`\n",
    "# So if the key and the query are \"aligned\", then the weight will be high which means that\n",
    "# the token will be more likely to be attended to by the other token\n",
    "# We will also have a\n",
    "# 3. Value vector\n",
    "# Which is a representation of the token that we want to be propagated in case it's selected\n",
    "\n",
    "# So let's try to implement this and change the above code to use self-attention\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x) # (B, T, head_size)\n",
    "q = query(x) # (B, T, head_size)\n",
    "\n",
    "# So far, keys and queries have been independetly computed for each token\n",
    "# Now let's do the dot product between them\n",
    "\n",
    "# We're transposing last two dimensions of the key matrix\n",
    "wei = q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) -> (B, T, T)\n",
    "\n",
    "# Now that we've found the initial affinities, we need to do the same as before\n",
    "tril = torch.tril(torch.ones(T, T))\n",
    "# Remember, no communication from the future to the past, because we're doing next token prediction\n",
    "# That's why we mask the upper triangle\n",
    "# But if we wanted to do something like sentiment analysis with a transformer,\n",
    "# WE wouldn't need to mask the upper triangle, and we could just use the entire matrix\n",
    "# (Which would correspond to a fully connected graph where the nodes are the tokens)\n",
    "# In the case where we do mask, it's called a decoder block (decodes the future)\n",
    "# And in the case where we don't mask, it's called an encoder block (encodes everything)\n",
    "wei = wei.masked_fill(tril == 0, float(\"-inf\"))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "v = value(x)\n",
    "\n",
    "out = wei @ v\n",
    "\n",
    "# Previously, every batch had the same weights, but now\n",
    "# every batch has different weights, because every batch has differetn tokens\n",
    "wei \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, why do we keep calling this method self-attention? \n",
    "# (there's also something called the cross-attention!)\n",
    "# It's because all 3 of key, query and value come from the same source `x`.\n",
    "\n",
    "# For example, in encoder-decoder transformers, you can have a case where\n",
    "# the queries are produced by `x`, but the keys and values are produced by a whole different source\n",
    "# And sometimes from encoder blocks which encode some context that we'd like to condition on.\n",
    "# So basically, cross-attention is where we have another source of information (nodes)\n",
    "# And we'd like to pull that information into our current graph and use it there."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that in the \"Attention is all you need\" paper, they have a formula like this:\n",
    "\n",
    "$$\n",
    "Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "But here, we haven't used the denominator of the thing inside softmax, which is the square root of the head size. \\\n",
    "This normalization is called \"scaled attention\".\n",
    "\n",
    "The problem is that if you only have Gaussian inputs, there's going to be trouble with the variance, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = torch.randn(B, T, head_size)\n",
    "q = torch.randn(B, T, head_size)\n",
    "\n",
    "wei_1 = q @ k.transpose(-2, -1)\n",
    "wei_2 = wei_1 * head_size**(-0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9487)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0449)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(14.3682), tensor(0.8980))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei_1.var(), wei_2.var()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that the unnormalized `wei`'s variance scales with the `head_size`, \\\n",
    "but the normalized `wei`'s variance stays around the same as $1$.\n",
    "\n",
    "Now why is this important?\n",
    "\n",
    "You have seen that the `wei` feeds into the softmax, so it's very important (especially at initialization) that the `wei` be diffuse.\n",
    "If the values in `wei` take on some very positive or very negative numbers, then the softmax will be very peaked, and the weights will converge to one-hot vectors, which would mean that we're aggregating information from a single node, and that would be quite unfortunate!\n",
    "\n",
    "Let's see a quick example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]) * 8, dim=-1)\n",
    "\n",
    "# This is peaked at output[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    '''One head of self-attention'''\n",
    "\n",
    "    def __init__(self, n_embed: int, head_size: int):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_size = head_size\n",
    "        \n",
    "        self.key = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embed, head_size, bias=False)\n",
    "\n",
    "        # Here we're creating this tril variable, but `tril` is not a parameter of the module\n",
    "        # So in the pythonic convention, it's called a \"buffer\" and not a \"parameter\".\n",
    "        # Basically this is assigning `tril` to `self` but telling Pytorch\n",
    "        # that it's not a real parameter of the module, so don't optimize it or anything.\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)   # B, T, C\n",
    "        q = self.query(x) # B, T, C\n",
    "\n",
    "        # compute scores (affinities)\n",
    "        wei = q @ k.transpose(-2, -1) * self.head_size**(-0.5) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\")) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "\n",
    "        # perform the weighted aggergation of the values\n",
    "        v = self.value(x) # (B, T, C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model even more with self-attention!\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int, head_size: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # sa = self-attention\n",
    "        self.sa_head = Head(n_embed, head_size)\n",
    "\n",
    "        # We have changed our channel size to be n_embed, but we wanted to get logits\n",
    "        # in such a way that the logits are in the same shape as the vocabulary\n",
    "        # because they are the probabilities of each token in the vocabular for being the next token\n",
    "        # So we add a linear layer to transform the channel size to be the same as the vocab size\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        # lm = language model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "        x = self.sa_head(x) # apply one head of self-attention (B, T, C)\n",
    "\n",
    "        # So now, the x will not only hold the token identity, but also the position of the token\n",
    "        # It is currently not that useful because we're only using the bigram model,\n",
    "        # so it's all translation invariant at this stage. (Doesn't matter _where_ the token is)\n",
    "        # But this will be useful when we use the transformer model (self-attention)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # Crop the idx to last_block_size tokens \n",
    "            # Because now we're using positional embeddings, we can never have\n",
    "            # more than `block_size` coming in.  \n",
    "            # The positional embedding has embeddings only up to `block_size`. (# of rows)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to do something called the multi-head attention. \\\n",
    "This is basically doing a stack of parallel attention layers, and then concatenating the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a stack of parallel attention layers\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embed:int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(n_embed, head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def foward(self, x: torch.tensor):\n",
    "        # Concatenate the results of all the heads\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model even more with self-attention!\n",
    "# We will use the MultiHeadAttention class that we just made\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # sa = self-attention\n",
    "        # We make each head smaller corresponding to the number of heads\n",
    "        # So that the total number of parameters is the same as the original\n",
    "        # This is like doing a group convolution instead of a large convolution\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, n_embed // n_heads, n_embed)\n",
    "\n",
    "        # We have changed our channel size to be n_embed, but we wanted to get logits\n",
    "        # in such a way that the logits are in the same shape as the vocabulary\n",
    "        # because they are the probabilities of each token in the vocabular for being the next token\n",
    "        # So we add a linear layer to transform the channel size to be the same as the vocab size\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        # lm = language model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "        x = self.sa_heads(x) # apply one head of self-attention (B, T, C)\n",
    "\n",
    "        # So now, the x will not only hold the token identity, but also the position of the token\n",
    "        # It is currently not that useful because we're only using the bigram model,\n",
    "        # so it's all translation invariant at this stage. (Doesn't matter _where_ the token is)\n",
    "        # But this will be useful when we use the transformer model (self-attention)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # Crop the idx to last_block_size tokens \n",
    "            # Because now we're using positional embeddings, we can never have\n",
    "            # more than `block_size` coming in.  \n",
    "            # The positional embedding has embeddings only up to `block_size`. (# of rows)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look at the \"attention is all you need\" paper, you'll see that they also have a feed-forward layer,\n",
    "after two levels of \"masked multi-head attention\", which basically gives tokens more chance to think about each other.\n",
    "\n",
    "![](.graphics/2023-04-21-20-26-17.png)\n",
    "\n",
    "Basically this, but ignoring the left part (we'll get back to it later he said)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, n_embed),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model even more with self-attention!\n",
    "# We will use the MultiHeadAttention class that we just made\n",
    "# And now we add the FeedForward network as well!\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # sa = self-attention\n",
    "        # We make each head smaller corresponding to the number of heads\n",
    "        # So that the total number of parameters is the same as the original\n",
    "        # This is like doing a group convolution instead of a large convolution\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        self.sa_heads = MultiHeadAttention(n_heads, head_size, n_embed)\n",
    "\n",
    "        # This is done on a token-level basis.\n",
    "        # What it means that each token has gathered all the information\n",
    "        # and now each token needs to think on that data individually\n",
    "\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "\n",
    "        # We have changed our channel size to be n_embed, but we wanted to get logits\n",
    "        # in such a way that the logits are in the same shape as the vocabulary\n",
    "        # because they are the probabilities of each token in the vocabular for being the next token\n",
    "        # So we add a linear layer to transform the channel size to be the same as the vocab size\n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "        # lm = language model\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "        x = self.sa_heads(x) # apply one head of self-attention (B, T, C)\n",
    "\n",
    "        x = self.ffwd(x) # (B, T, C)\n",
    "\n",
    "        # So now, the x will not only hold the token identity, but also the position of the token\n",
    "        # It is currently not that useful because we're only using the bigram model,\n",
    "        # so it's all translation invariant at this stage. (Doesn't matter _where_ the token is)\n",
    "        # But this will be useful when we use the transformer model (self-attention)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # Crop the idx to last_block_size tokens \n",
    "            # Because now we're using positional embeddings, we can never have\n",
    "            # more than `block_size` coming in.  \n",
    "            # The positional embedding has embeddings only up to `block_size`. (# of rows)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to now intersperse the communication with the computation!\n",
    "\n",
    "Now we implement something called the block, which is the \"gray\" box in the picture above. (except for the cross-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        # Communication is the MultiHeadAttention\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size, n_embed)\n",
    "        \n",
    "        # Computation is the FeedForward\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        x = self.sa(x)\n",
    "        x = self.ffwd(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's upgrade the Bigram model ONCE AGAIN.\n",
    "\n",
    "(As you can see, there is a $\\times N$ thing next to the graph, which means we want to use $N$ layers of the block.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model even more with Blocks!\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # A couple of blocks in sequence (Communication and Computation, many times)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4),\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "        \n",
    "        x = self.blocks(x) # (B, T, n_embed)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # Crop the idx to last_block_size tokens \n",
    "            # Because now we're using positional embeddings, we can never have\n",
    "            # more than `block_size` coming in.  \n",
    "            # The positional embedding has embeddings only up to `block_size`. (# of rows)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above model actually doesn't give out good results, because it's actually a _deep_ neural net. And deep NNs suffer from optimization problems.\n",
    "\n",
    "So we need **one** more idea that we can burrow from the Transformer paper to resolve this issue."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, there are two optimizations that dramatically help with the depth of these networks and make sure that the networks remain optimizable.\n",
    "\n",
    "1. Residual connections:\n",
    "\n",
    "Look at the three arrows highlighted in the picture below:\n",
    "\n",
    "![](.graphics/2023-04-24-18-05-16.png)\n",
    "\n",
    "Those are called the \"skip\" or the \"residual\" connections.\n",
    "\n",
    "They come from this paper: [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) \\\n",
    "(Holy shit co-pilot figured out the paper name and url on its own!)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the figures of this [Towards Datascience post](https://towardsdatascience.com/residual-blocks-building-blocks-of-resnet-fd90ca15d6ec),\n",
    "\n",
    "![](.graphics/2023-04-24-18-11-29.png)\n",
    "\n",
    "It's like we have a high-way from our inputs data directly to our output data using addition.\n",
    "\n",
    "Basically, there is going to be a fork in the computation path that doesn't go through all of the \"extra\" computation done by a specific part of the NN, so it will be much better at preserving the gradient it's going to be carrying through.\n",
    "\n",
    "With that knowledge, let's update our Block class!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, n_embed: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embed, 4 * n_embed),\n",
    "            nn.ReLU(),\n",
    "            # This is the projection layer \"going back into the residual pathway\"\n",
    "            # (in quotes because I don't totally understand it)\n",
    "            nn.Linear(4 * n_embed, n_embed),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.tensor):\n",
    "        return self.net(x)\n",
    "\n",
    "# Why the 4 * n_embed?\n",
    "# Because in the paper, they use a 512 dimensionality for the input\n",
    "# and 2048 for the hidden layer, so 4 * 512 = 204\n",
    "# So we use the same ratio here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also need to update this to add the \"accumulative\" NN\n",
    "# at the end of it, which in a way gets a \"summary\" of the parallel \n",
    "# computations we do inside of it.\n",
    "\n",
    "# a stack of parallel attention layers\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads: int, head_size: int, n_embed:int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            Head(n_embed, head_size) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.proj = nn.Linear(n_embed, n_embed)\n",
    "\n",
    "    \n",
    "    def foward(self, x: torch.tensor):\n",
    "        # Concatenate the results of all the heads\n",
    "        out = torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "        out = self.proj(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        # Communication is the MultiHeadAttention\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size, n_embed)\n",
    "        \n",
    "        # Computation is the FeedForward\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        # Adding the residual connection\n",
    "        x = x + self.sa(x)\n",
    "        # To both communication and computation\n",
    "        x = x + self.ffwd(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point in the video (around here: https://youtu.be/kCc8FmEb1nY?t=5646)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "He started by talking about batch normalization, so I watched Andrew Ng's videos on that.\n",
    "Then I watched Yannic Kilcher's video on [Group Normalization](https://www.youtube.com/watch?v=l_3zj6HeWUE).\n",
    "\n",
    "I have written a bunch of notes on batch normalization in the `Ng - Improving DNNs` folder, but I'll just write down the forumla for **Layer Normalization** here: \\\n",
    "(We compute the layer normalization statistics over all the hidden units in the same layer)\n",
    "\n",
    "$$\n",
    "\\mu^{[l]} = \\frac{1}{H} \\sum_{i=1}^{H} z^{[l](i)} \\\\\n",
    "\\, \\\\\n",
    "\\sigma^{[l]} = \\sqrt{\\frac{1}{H} \\sum_{i=1}^{H} (z^{[l]}_{i} - \\mu^{[l]})^2} \\\\\n",
    "$$\n",
    "\n",
    "Where $H$ is the number of hidden units in the layer, and $z^{[l]}_{i}$ is the $i$ th hidden unit in the $l$ th layer.\n",
    "\n",
    "(There is also instance normalization and group normalization, but we don't need to worry about those for now.)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, for the second method of improving our optimization:\n",
    "\n",
    "2. Layer Normalization:\n",
    "\n",
    "We see in our graph above that there is an `Add & Norm` block, and we want to use Layer Normalization for it, but the slight change that has happened in Transformer implementations is that nowadays people do the `LayerNorm` before the `MaskedMultiHeadAttention` and the `FeedForward` layers, and then do the `Add` after them. This is called the \"pre-norm\" formulation.\n",
    "\n",
    "So let's update our Block once again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        head_size = n_embed // n_heads\n",
    "\n",
    "        # Communication is the MultiHeadAttention\n",
    "        self.sa = MultiHeadAttention(n_heads, head_size, n_embed)\n",
    "        \n",
    "        # Computation is the FeedForward\n",
    "        self.ffwd = FeedForward(n_embed)\n",
    "\n",
    "        # So both batch and time dimensions act as batch dimensions\n",
    "        # (Because we're passing n_embed as the number of features here)\n",
    "        self.ln1 = nn.LayerNorm(n_embed)\n",
    "        self.ln2 = nn.LayerNorm(n_embed)\n",
    "    \n",
    "    def forward(self, x: torch.tensor):\n",
    "        # Adding the residual connection\n",
    "        x = x + self.sa( self.ln1(x) )\n",
    "        # To both communication and computation\n",
    "        x = x + self.ffwd( self.ln2(x) )\n",
    "\n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add the LayerNorm to our Bigram model as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's try to update the Bigram model even more with Blocks!\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int, block_size: int, n_embed: int, n_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        # We make an embedding table for the __value__ of the tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embed)\n",
    "\n",
    "        # We also make an embedding table for the __position__ of the tokens\n",
    "        # meaning that both the token and the position of the token will be paid attention to\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embed)\n",
    "\n",
    "        # A couple of blocks in sequence (Communication and Computation, many times)\n",
    "\n",
    "        self.blocks = nn.Sequential(\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4),\n",
    "            Block(n_embed, n_heads=4),\n",
    "            # We should also have a LayerNorm at the end of the stack\n",
    "            # Right before linear layer that decodes into the vocabulary\n",
    "            nn.LayerNorm(n_embed)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(n_embed, vocab_size)\n",
    "    \n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B, T) tensors of integers\n",
    "\n",
    "        tok_emb = self.token_embedding_table(idx) # (B, T, n_embed)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=idx.device)) # (T, n_embed)\n",
    "\n",
    "        # The batch dimension will get automatically broadcasted\n",
    "        # for the position embedding, as it should be the same for all the batches\n",
    "        x = tok_emb + pos_emb # (B, T, n_embed)\n",
    "        \n",
    "        x = self.blocks(x) # (B, T, n_embed)\n",
    "\n",
    "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
    "\n",
    "    ### From here on out, it's the same as the previous version\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # Crop the idx to last_block_size tokens \n",
    "            # Because now we're using positional embeddings, we can never have\n",
    "            # more than `block_size` coming in.  \n",
    "            # The positional embedding has embeddings only up to `block_size`. (# of rows)\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "\n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx_cond)\n",
    "\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, he adds a bunch of dropout layers, changes the number of layers in each block, and changes the number of heads in the multi-head attention and batch size, and he runs it on his `a100` GPU for 15 mins, and gets a pretty good structure-wise (but not-semantically) correct sentences."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-Decoder Architecture\n",
    "\n",
    "What we implemented above (the left hand side of the graphic) is called the Decoder part of the Transformer, we didn;t implement the right-hand side because this is just a text-prediction model, and we have that upper-triangular mask that prevents the model from looking at the future tokens, which makes it a decoder.\n",
    "\n",
    "The task considered in the original paper is a translation task, and that's why they needed the Encoder part of the transformer. (Encodes French, decodes English)\n",
    "\n",
    "What it means is that the generation will be conditioned on the French sentence, and now all the tokens will be allowed to talk to each other (No mask)\n",
    "\n",
    "So the queries are still generated from (left hand side) `x`, but the keys and the values are coming from the right hand side, so it conditions the generation on the entirety of the French sentence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nano-GPT\n",
    "\n",
    "It's basically what we have done here, and it includes two files `train.py` and `model.py`. \n",
    "\n",
    "`model.py` is basically the same code as here, but it's more complicated because it includes:\n",
    "\n",
    "- Saving and loading checkpoints\n",
    "- Pre-trained weights\n",
    "- Decaying the learning rate\n",
    "- Compiling the model\n",
    "- Using distributed learning across multiple nodes (GPUs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat-GPT\n",
    "\n",
    "Chat-GPT was pre-trained like this on a large corpus of the internet (You can see model parameters and data-size in their paper), but then it was fine-tuned to be a chatbot.\n",
    "\n",
    "Then they got some human evaluators to evaluate the chatbot, and used that human-feedback data to train a reward model.\n",
    "\n",
    "Once they have the reward model, they run PPO, which is a form of policy gradient reinforcement learning optimizer to fine-tune their sampling policy (The answers that chatGPT generates are expected to score a high reward from the reward model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
