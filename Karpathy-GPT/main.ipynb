{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT = Generative Pre-Trained Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the data into a string\n",
    "with open(\"tinyshakespeare.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "data_size, vocab_size = len(text), len(chars)\n",
    "print('data has %d characters, %d unique.' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text into words\n",
    "stoi = {ch: i for i, ch in enumerate(chars)}\n",
    "itos = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# Convert the text into a sequence of integers\n",
    "encoder = lambda s: [stoi[c] for c in s]\n",
    "decoder = lambda s: ''.join([itos[c] for c in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1115394]) torch.int64\n",
      "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
      "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
      "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
      "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
      "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
      "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "data = torch.tensor(encoder(text), dtype=torch.int64)\n",
    "print(data.shape, data.dtype)\n",
    "print(data[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Split the data into train and test\n",
    "n_train = int(0.9 * data_size)\n",
    "train_data = data[:n_train]\n",
    "test_data = data[n_train:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not going to feed the entire sequence into the model at once. \\\n",
    "Instead, we'll feed it a small chunk at a time, and then let it continue predicting the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sometimes called the chunk size or batch size.\n",
    "block_size = 8\n",
    "\n",
    "# We will actually separate block_size + 1 characters at a time. \n",
    "# Look at it this way:\n",
    "\n",
    "# For block[0],    we will predict block[1], \n",
    "# For block[0, 1], we will predict block[2], \n",
    "# and so on. \n",
    "\n",
    "# Which means we need to have block[8] as target for block[0, ... , 7]\n",
    "\n",
    "# This is not just done for efficiency, but also to make sure that the \n",
    "# Transformer gets used to seeing contexts as little as 1 character long \n",
    "# and as long as block_size c haracters long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When input is [18], the target is 47.\n",
      "When input is [18, 47], the target is 56.\n",
      "When input is [18, 47, 56], the target is 57.\n",
      "When input is [18, 47, 56, 57], the target is 58.\n",
      "When input is [18, 47, 56, 57, 58], the target is 1.\n",
      "When input is [18, 47, 56, 57, 58, 1], the target is 15.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15], the target is 47.\n",
      "When input is [18, 47, 56, 57, 58, 1, 15, 47], the target is 58.\n"
     ]
    }
   ],
   "source": [
    "x = train_data[ : block_size]\n",
    "y = train_data[1: block_size + 1]\n",
    "\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"When input is {context.tolist()}, the target is {target}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There's one more thing to care about and that's the batch dimension.\n",
    "# We want to be able to feed in a batch of sequences at a time (as a tensor) to the GPU\n",
    "# and have it process all of them in parallel for a speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a seed to make sure the same pseudo-random sequence is generated every time we run this cell\n",
    "torch.manual_seed(1337)\n",
    "batch_size = 4      # How many independent sequences to process in parallel\n",
    "block_size = 8      # Maximum context length for predictions\n",
    "\n",
    "\n",
    "def get_batch(split: str):\n",
    "    # generate a small batch of data of inputs `x` and targets `y`\n",
    "    data = train_data if split == \"train\" else test_data\n",
    "    # stochastic sampling of the data, ix = indices\n",
    "    ix = torch.randint(len(data) - block_size, size=(batch_size,))\n",
    "    x = torch.stack([data[i : i + block_size] for i in ix])\n",
    "    y = torch.stack([data[i + 1 : i + block_size + 1] for i in ix])\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "---\n",
      "When input is [24], the target is 43.\n",
      "When input is [24, 43], the target is 58.\n",
      "When input is [24, 43, 58], the target is 5.\n",
      "When input is [24, 43, 58, 5], the target is 57.\n",
      "When input is [24, 43, 58, 5, 57], the target is 1.\n",
      "When input is [24, 43, 58, 5, 57, 1], the target is 46.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46], the target is 43.\n",
      "When input is [24, 43, 58, 5, 57, 1, 46, 43], the target is 39.\n",
      "When input is [44], the target is 53.\n",
      "When input is [44, 53], the target is 56.\n",
      "When input is [44, 53, 56], the target is 1.\n",
      "When input is [44, 53, 56, 1], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58], the target is 46.\n",
      "When input is [44, 53, 56, 1, 58, 46], the target is 39.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39], the target is 58.\n",
      "When input is [44, 53, 56, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52], the target is 58.\n",
      "When input is [52, 58], the target is 1.\n",
      "When input is [52, 58, 1], the target is 58.\n",
      "When input is [52, 58, 1, 58], the target is 46.\n",
      "When input is [52, 58, 1, 58, 46], the target is 39.\n",
      "When input is [52, 58, 1, 58, 46, 39], the target is 58.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58], the target is 1.\n",
      "When input is [52, 58, 1, 58, 46, 39, 58, 1], the target is 46.\n",
      "When input is [25], the target is 17.\n",
      "When input is [25, 17], the target is 27.\n",
      "When input is [25, 17, 27], the target is 10.\n",
      "When input is [25, 17, 27, 10], the target is 0.\n",
      "When input is [25, 17, 27, 10, 0], the target is 21.\n",
      "When input is [25, 17, 27, 10, 0, 21], the target is 1.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1], the target is 54.\n",
      "When input is [25, 17, 27, 10, 0, 21, 1, 54], the target is 39.\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "\n",
    "# Print an example batch\n",
    "xb, yb = get_batch(\"train\")\n",
    "\n",
    "print(\"inputs:\")\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "\n",
    "print(\"targets:\")\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print(\"---\")\n",
    "\n",
    "for b in range(batch_size):\n",
    "    for t in range(block_size):\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b, t]\n",
    "        print(f\"When input is {context.tolist()}, the target is {target}.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Logit function, also known as the log-odds function, is a function that represents probability values from 0 to 1, and negative infinity to infinity. The function is an inverse to the sigmoid function that limits values between 0 and 1 across the Y-axis, rather than the X-axis.\n",
    "\n",
    "$$\n",
    "logit(p) = \\log(\\frac{p}{1-p})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "    \n",
    "    def forward(self, idx: torch.tensor, targets: torch.tensor = None) -> torch.tensor:\n",
    "\n",
    "        # idx and targets are both (Batch, Time) tensors of integers\n",
    "        logits = self.token_embedding_table(idx) # (Batch, Time, Channel)\n",
    "\n",
    "        # When we pass our input through the embedding table, \n",
    "        # every single integer in our input is going to refer to this embedding table\n",
    "        # is going to pluck out a row from this embedding table corresponding to that integer (as an index)\n",
    "        # In this case, Batch = 4, Time = 8, Channel = 65 (vocab_size)\n",
    "\n",
    "        # Logits are the scores for the next character in the sequence\n",
    "        # As you see, this is a bi-gram, which means that the next character is predicted\n",
    "        # Only based the current character, and not the entire sequence\n",
    "\n",
    "        # Aka negative log likelihood loss\n",
    "        # loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        # The above code won't work, because we have a tensor in the shape of (B, T, C)\n",
    "        # But Pytorch's cross entropy expects a tensor of shape (B * T, C)\n",
    "        # So basically we need to flatten the input tensor and the target tensor into a 1D tensor\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "\n",
    "            # We will reshape the logits instead of the input here.\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "        \n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.tensor, max_new_tokens: int):\n",
    "        # idx is a (Batch, Time) array of indices in the current context\n",
    "\n",
    "        # The point of this is to start with a sequence and \n",
    "        # keep adding new tokens to it, and then keep feeding it back into the model\n",
    "        # to get new predictions until we have the desired number of new tokens\n",
    "\n",
    "        for _ in range(max_new_tokens):\n",
    "            \n",
    "            # This function, even though general, is a bit ridiculous\n",
    "            # Because this is a bi-gram model, we only need to pass the last token\n",
    "            # to our table, but we are passing the entire sequence to the table\n",
    "            # and then, at the next step (`logits = ...`) we pick the last token\n",
    "            # Out of all the tokens in the sequence\n",
    "\n",
    "            # Later, we will use the entire sequence to predict the next token\n",
    "             \n",
    "            # get the predictions, we don't care about the loss because we're not training\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (Batch, Channel)\n",
    "            # apply softmax to convert to probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # becomes (Batch, Channel)\n",
    "            # sample from the distribution or take the most likely\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # becomes (Batch, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat([idx, idx_next], dim=-1) # becomes (Batch, Time + 1)\n",
    "        \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.5262, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "m = BigramLanguageModel(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, because initally our model should equally predict any character as the next character\n",
    "# Our log likelihood loss should be around -ln(1/65) = 4.174387269895637"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "'JgC.JZWqUkpdtkSpmzjM-,RqzgaN?vC:hgjnAnBZDga-APqGUH!WdCbIb;$DefOYbEvcaKGMmnO'q$KdS-'ZH\n",
      ".YSqr'X!Q! d;\n"
     ]
    }
   ],
   "source": [
    "# We set our initial token to be 0, which is the newline character in our vocabulary\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# We have one batch and the [0] is picking the first batch\n",
    "print(decoder(m.generate(idx, max_new_tokens=100)[0].tolist()))\n",
    "\n",
    "# The model is obviously not trained so it's just spitting out random characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, loss = 3.6967320442199707\n",
      "Step 1000, loss = 3.0273618698120117\n",
      "Step 2000, loss = 2.821552276611328\n",
      "Step 3000, loss = 2.547393798828125\n",
      "Step 4000, loss = 2.519805908203125\n",
      "Step 5000, loss = 2.529714584350586\n",
      "Step 6000, loss = 2.542541980743408\n",
      "Step 7000, loss = 2.5140950679779053\n",
      "Step 8000, loss = 2.5017194747924805\n",
      "Step 9000, loss = 2.5214755535125732\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_steps = 10000\n",
    "\n",
    "for steps in range(max_steps):\n",
    "    # get a batch of data\n",
    "    xb, yb = get_batch(\"train\")\n",
    "\n",
    "    # get the model predictions and loss\n",
    "    logits, loss = m(xb, yb)\n",
    "\n",
    "    # zero out the gradients\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # compute the gradients\n",
    "    loss.backward()\n",
    "\n",
    "    # update the model parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    if steps % 1000 == 0:\n",
    "        print(f\"Step {steps}, loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CKELOresm, bur stthakls,\n",
      "Ther layo-then ha nleincede jahe\n",
      "DZW:\n",
      "Gothe s kendwepive.\n",
      "FAnorereroldghmig ppu:\n",
      "Co nlllinger hus;\n",
      "aver his, t towis t s ng,\n",
      "ANE: foratreaisplblthriat, otimust hiny ille, yomeON p, IN I ckist vemo th.\n",
      "Dieathy al hi?\n",
      "Fo'd ha s?\n",
      "ARS:\n",
      "Semnd thinghy.\n",
      "IORDitwint sth! mine actwis \n"
     ]
    }
   ],
   "source": [
    "# We set our initial token to be 0, which is the newline character in our vocabulary\n",
    "\n",
    "idx = torch.zeros((1, 1), dtype=torch.long)\n",
    "\n",
    "# We have one batch and the [0] is picking the first batch\n",
    "print(decoder(m.generate(idx, max_new_tokens=300)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is obviously not shakespeare, but it's a lot better than random characters!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop for now at Chapter (Port our code to a script) (38:00)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
