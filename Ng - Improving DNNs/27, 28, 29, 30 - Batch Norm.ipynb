{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27: [Normalizing Activations](https://youtu.be/tNIpEZLv_eg)\n",
    "\n",
    "## 28: [Fitting Batch Norm into Neural Networks](https://youtu.be/em6dfRxYkYU)\n",
    "\n",
    "## 29: [Why Does Batch Norm Work?](https://youtu.be/nUUqwaxLnWs)\n",
    "\n",
    "## 30: [Batch Norm at Test Time](https://youtu.be/5qefnAek8OA)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "\n",
    "The basic idea is, we normalized our data inputs in Video 9 to make the loss landscape easier to navigate. We can do the same thing for the activations (outputs) $a^{[n]}$ of our hidden layers, so the next layer has a more normalized input, and therefore its weights can be more easily optimizer. This is called **Batch Normalization**.\n",
    "\n",
    "Now, there is debate on whether to apply the batch normalization before or after the activation function. In practice, normalizing the output before the activation function $z^{[n]}$ is more common. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Batch Normalization\n",
    "\n",
    "Given some intermediate values at some layer $l$ in the NN, $z^{(1)}, \\dots, z^{(m)}$, we want to normalize them. (This could be written as $z^{[l](1)}, \\dots, z^{[l](m)}$ if we want to be more explicit about the layer, but we omit it here for clarity.)\n",
    "\n",
    "We define the mean and variance of the batch as:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^m z^{(i)} \\\\\n",
    "\\, \\\\\n",
    "\\sigma^2 = \\frac{1}{m} \\sum_{i=1}^m (z^{(i)} - \\mu)^2\n",
    "$$\n",
    "\n",
    "And we normalize the batch as:\n",
    "\n",
    "$$\n",
    "z^{(i)}_{norm} = \\frac{z^{(i)} - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "$$\n",
    "\n",
    "Where $\\epsilon$ is a small number to avoid division by zero.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, every component of $z$ has mean 0 and variance 1. But we don't want the hidden units to always have mean 0 and variance 1, as it makes sense for them to have a different distribution, so we add two new parameters $\\gamma$ and $\\beta$ to the layer, and we define the \"corrected\" values as:\n",
    "\n",
    "$$\n",
    "\\tilde{z}^{(i)} = \\gamma \\times z^{(i)}_{norm} + \\beta\n",
    "$$\n",
    "\n",
    "And here, $\\gamma$ and $\\beta$ are learnable parameters of the model, and we pass the $\\tilde{z}^{(i)}$ to the activation function.\n",
    "\n",
    "Note that we have a unique $\\gamma$ and $\\beta$ for each layer of the NN."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Normalization in Practice\n",
    "\n",
    "In practice, batch norm is usually applied with mini-batches of your training set. \\\n",
    "(And also, doing the batch-norm factors out the bias $b$ parameters of each layer, so we can just omit them.)\n",
    "\n",
    "Also note that the dimension of $\\gamma^{(i)}$ and $\\beta^{(i)}$ is the same as the number of neurons in the layer. (each neuron gets a scalar $\\gamma$ and $\\beta$)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intuition for Batch Norm\n",
    "\n",
    "The reason number $1$ for why it works is the same argument as the one for why normalizing the inputs works: it makes the loss landscape easier to navigate, and therefore the weights are easier to optimize.\n",
    "\n",
    "Now, To get a better idea about the intuition of the other reason, let's look at an example:\n",
    "\n",
    "If you have a NN for figuring out if a picture is a cat picture or not, your model will probably not be able to generalize well to a new dataset if the training set only has pictures of black cats, and no pictures of colored cats.\n",
    "\n",
    "This idea of your data distribution changing goes by the name of **Covariate Shift**.\n",
    "\n",
    "And it says that if you learn some $X \\mapsto Y$ mapping, if the distribution of $X$ changes, you need to retrain your model. And this is true, even if your ground truth function mapping from $X$ to $Y$ (cat pictures to outputting \"cat\") doesn't change. And the need to retrain your function gets worse if your ground truth changes as well.\n",
    "\n",
    "So the $2^{nd}$ reason why Batch Norm works is that it makes weights that come later in your NN less dependent (or more robust to changes) on the weights that come earlier. \\\n",
    "This is because for each layer, basically the output of their previous layer is their input data, doing this batch norm makes it such that the input data (relative to that layer) is normalized, and therefore the weights of that layer don't have to change as much to adapt to changes in the previous layer. \n",
    "\n",
    "Pictures:\n",
    "\n",
    "![](.graphics/2023-04-26-21-06-32.png)\n",
    "\n",
    "![](.graphics/2023-04-26-21-06-42.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm as Regularization\n",
    "\n",
    "We know that each mini-batch is scaled by the mean/variance computed on _just that_ mini-batch.\n",
    "\n",
    "This (similar to dropout) adds some noise to the values $z^{[l]}$ within that mini-batch, because the mean and variance we compute from that mini-batch aren't the true mean and variance of the entire training set.\n",
    "\n",
    "So by adding noise to our hidden units, this is forcing the downstream hidden units to rely not too much on any one unit that comes before them. Because the noise is comparatively small, this is not going to be a huge regularizing effect, but it is there.\n",
    "\n",
    "So you would also reduce your regularization if you increase the size of your mini-batch.\n",
    "\n",
    "Having said that, you should not rely on batch norm as a regularization technique, and you should still use other methods."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Norm at Test Time\n",
    "\n",
    "This version of batch normalization will not make sense during test time, because you might be processing one data at a time, so the scaling will be nonsense. \n",
    "\n",
    "So we need to come up with some separate estimate of $\\mu$ and $\\sigma^2$ to use at test time. \\\n",
    "In typical implementations of the batch norm, this is done using an exponentially weighted average of the mean and variance of the mini-batches. (for each layer)\n",
    "\n",
    "And then, we use these exponential averages to normalize the test data.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
