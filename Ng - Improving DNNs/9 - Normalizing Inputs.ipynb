{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video 9: [Normalizing Inputs](https://youtu.be/FDCfw-YqWTE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we have a dataset like this (the intuition could be easily extended to more dimensions):\n",
    "\n",
    "![](.graphics/2023-04-26-17-20-06.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a reason that we will get to shortly, we want to Normalize the data. This means that we want to transform the data so that it has a mean of 0 and a variance of 1. (Turn it into a standard normal distribution).\n",
    "\n",
    "So assuming our data points are $\\{ x^{(1)}, \\dots, x^{(m)} \\}$ we calculate the following values:\n",
    "\n",
    "$$\n",
    "\\mu = \\frac{1}{m} \\sum_{i=1}^{m} x^{(i)} \\\\\n",
    "\\, \\\\\n",
    "\\sigma^2 = \\frac{1}{m - 1} \\sum_{i=1}^{m} (x^{(i)} - \\mu)^2\n",
    "$$\n",
    "\n",
    "(Remember, dividing by $m-1$ instead of $m$ makes the sample variance an unbiased estimator of the population variance. Also, the square is element-wise).\n",
    "\n",
    "And with this, we can normalize the data:\n",
    "\n",
    "$$\n",
    "x^{(i)}_{norm} = \\frac{x^{(i)} - \\mu}{\\sigma}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Normalize inputs?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your input is unnormalized, in case where the variance of different features in the input is very different, the loss function will be very elongated and it will take a long time for gradient descent to converge.\n",
    "\n",
    "Loss function will look something like this:\n",
    "\n",
    "![](.graphics/2023-04-26-17-33-20.png)\n",
    "\n",
    "And its contours will look like this:\n",
    "\n",
    "![](.graphics/2023-04-26-17-36-36.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we change the input to be normalized, the loss function will look like this:\n",
    "\n",
    "![](.graphics/2023-04-26-17-37-16.png)\n",
    "\n",
    "And its contours will look like this:\n",
    "\n",
    "![](.graphics/2023-04-26-17-37-43.png)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that in the unnormalized case, gradient descent will take a long time to converge because it will take a long time to get to the bottom of the valley. But in the normalized case, gradient descent will converge much faster.\n",
    "\n",
    "In the unnormalized case:\n",
    "\n",
    "![](.graphics/2023-04-26-17-38-50.png)\n",
    "\n",
    "In the normalized case:\n",
    "\n",
    "![](.graphics/2023-04-26-17-39-14.png)\n",
    "\n",
    "Remember (from Grant Sanderson's lectures on multivariable calculus), that the gradient is always orthogonal to the contour lines.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Doing this normalization pretty much never does any harm, so you can do it even if the ranges of your data's features are similar."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
